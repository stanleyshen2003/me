import React from "react";
import notion_styles from "../notion";

function HCISPaper() {
	const basedir = process.env.PUBLIC_URL || "/me";

	return {
		date: "08 July 2025",
		title: "å¤§å°ˆç”Ÿ & Paper",
		description:
			"å¤§å°ˆç”Ÿç ”ç©¶è¨ˆç•« & Paper in HCIS Lab çš„ä¸€äº›å°å¿ƒå¾—ã€‚",
		type: "School",
		style: notion_styles,
		keywords: [
			"Paper",
            "å¤§å°ˆç”Ÿ",
            "Research",
			"Stanley",
			"Stanley Shen",
		],
		body: (
			<React.Fragment>
				<article id="22ab7849-9a0a-80e8-b706-d6b62bd3cd4d" class="page sans">
                    <header>
                        {/* <h1 class="page-title">å¤§å°ˆç”Ÿè¨ˆç•« &amp; è«–æ–‡</h1> */}
                        <p class="page-description"></p>
                    </header>
                    <div class="page-body">
                        <p id="22ab7849-9a0a-80ca-8e0f-d51818d4e6f1" class="">å‰æƒ…æè¦ä¸€ä¸‹ï¼Œæˆ‘çš„å°ˆé¡Œåšçš„æ˜¯æ©Ÿå™¨äºº@<a href="https://sites.google.com/site/yitingchen0524/hcis-lab">HCIS LAB</a>ï¼ŒæŒ‡å°æ•™æˆæ˜¯é™³å¥•å»·æ•™æˆï¼Œä»¥ä¸‹ç°¡ç¨± YTã€‚</p>
                        <h1>Table of contents</h1>
                        <p></p>
                        <nav id="22ab7849-9a0a-80dc-a6df-d093e1e8a937" class="block-color-gray table_of_contents">
                            <div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href={`${basedir}/article/3#22ab7849-9a0a-80d3-8cd1-ebe79ad3f55d`}>å‰æƒ…æè¦</a></div>
                            <div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href={`${basedir}/article/3#22ab7849-9a0a-80ab-b0c3-c1fc439f297c`}>Links</a></div>
                            <div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href={`${basedir}/article/3#22ab7849-9a0a-80b9-a416-dad757cfe19c`}>æ™‚ç¨‹</a></div>
                            <div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href={`${basedir}/article/3#22ab7849-9a0a-8011-81ab-d87e0480d2b5`}>å¤§å°ˆç”Ÿ</a></div>
                            <div class="table_of_contents-item table_of_contents-indent-0"><a class="table_of_contents-link" href={`${basedir}/article/3#22ab7849-9a0a-801a-81e3-ed60f1ee3934`}>è«–æ–‡</a></div>
                            <div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href={`${basedir}/article/3#22ab7849-9a0a-80c0-a719-fc1dcda17747`}>èµ·å› </a></div>
                            <div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href={`${basedir}/article/3#22ab7849-9a0a-8050-8b9a-d73d31d9fbde`}>éç¨‹</a></div>
                            <div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href={`${basedir}/article/3#22ab7849-9a0a-80d1-9165-fa5f5c9836c3`}>æŠ•ç¨¿</a></div>
                            <div class="table_of_contents-item table_of_contents-indent-1"><a class="table_of_contents-link" href={`${basedir}/article/3#22ab7849-9a0a-802b-b4bd-de17275fc1ae`}>å¿ƒå¾—</a></div>
                        </nav>
                        <h2 id="22ab7849-9a0a-80ab-b0c3-c1fc439f297c" class="">Links</h2>
                        <p id="22ab7849-9a0a-8043-9367-f1b5af7773ce" class="">Source code - <a href="https://github.com/HCIS-Lab/Affordance-Guided-Self-Consistent-MLLM">HCIS-Lab/Affordance-Guided-Self-Consistent-MLLM</a></p>
                        <p id="22ab7849-9a0a-8023-b546-cd5ebcb0d217" class="">Project Page &amp; Paper - <a href="https://hcis-lab.github.io/Affordance-Guided-Self-Consistent-MLLM/">Here</a></p>
                        <h2 id="22ab7849-9a0a-80b9-a416-dad757cfe19c" class="">æ™‚ç¨‹</h2>
                        <p id="22ab7849-9a0a-8007-a4c0-dc8ce96cf25f" class="">2023.07 é–‹å§‹å°ˆé¡Œ</p>
                        <p id="22ab7849-9a0a-80b5-9937-e257c76938ae" class="">2024.01 ç”³è«‹å¤§å°ˆç”Ÿ</p>
                        <p id="22ab7849-9a0a-80d0-b838-dfa6f4d8e1dc" class="">2024.07 é–‹å§‹å¤§å°ˆç”Ÿ + å¼„è«–æ–‡</p>
                        <p id="22ab7849-9a0a-809d-ac3c-efb31ecf2a8b" class="">2024.03 æŠ•ç¨¿ IROS</p>
                        <h2 id="22ab7849-9a0a-8011-81ab-d87e0480d2b5" class="">å¤§å°ˆç”Ÿ</h2>
                        <p id="22ab7849-9a0a-801e-b23e-cd1fb96a679e" class="">åœ‹ç§‘æœƒå¤§å°ˆç”Ÿè¨ˆç•«ç®—æ˜¯è »å¤šå°ˆé¡Œç”Ÿæœƒç”³è«‹çš„ï¼Œå…¶å¯¦æœ€ä¸»è¦è¦åšçš„äº‹å°±æ˜¯æŠŠå°ˆé¡Œåšçš„äº‹å¥½å¥½æ•´ç†å‡ºè¨ˆç•«å’Œå ±å‘Šã€‚ä¸€å€‹æœˆè£œåŠ© $6000ï¼Œä½†å› ç‚ºæˆ‘è·Ÿæˆ‘éšŠå‹ä¸€èµ·ç”³è«‹çš„ï¼Œæ‰€ä»¥ä¸€å€‹æœˆå°±æ˜¯æ‹¿ $3000ã€‚é™¤äº†è£œåŠ©ä¹‹å¤–å…¶å¯¦é‚„ç”³è«‹äº† $10000 çš„é›œæ”¯é …ï¼Œæœ¬ä¾†æƒ³æŠŠå„² OpenAI API çš„éŒ¢å ±æ‰ï¼Œä½†å› ç‚ºè¦å ±å¸³çš„æ™‚å€™åœ¨å¿™è«–æ–‡ï¼Œå¿™å®Œæ‰ç™¼ç¾æ™‚é–“éäº† (my badâ€¦)ï¼Œæ‰€ä»¥å°±æ²’é ˜åˆ°ã€‚</p>
                        <h2 id="22ab7849-9a0a-801a-81e3-ed60f1ee3934" class="">è«–æ–‡</h2>
                        <h3 id="22ab7849-9a0a-80c0-a719-fc1dcda17747" class="">èµ·å› </h3>
                        <p id="22ab7849-9a0a-807c-a4f8-eb13640f365d" class="">é‚£æ™‚å€™ YT å°±å•æˆ‘å€‘èªªæˆ‘å€‘éƒ½èŠ±äº†é‚£éº¼å¤šæ™‚é–“ä¾†åšå¤§å°ˆç”Ÿï¼Œè¦ä¸è¦è©¦è‘—å¯« paper å»æŠ• conference or workshopã€‚é‚£æ™‚å€™æˆ‘å€‘å°± say yesï¼Œç„¶å¾Œé–‹å§‹èªçœŸåšç ”ç©¶ï¼Œpaper å¯«å®Œå¾Œå°±ç›´æ¥æ‹¿å»æ•™å¤§å°ˆç”Ÿç ”ç©¶è¨ˆç•«ã€‚</p>
                        <h3 id="22ab7849-9a0a-8050-8b9a-d73d31d9fbde" class="">éç¨‹</h3>
                        <p id="22ab7849-9a0a-804c-acf3-e0c84f9f4666" class="">æˆ‘è·Ÿæˆ‘éšŠå‹çš„åˆ†å·¥å¤§æ¦‚æ˜¯</p>
                        <ul id="22ab7849-9a0a-8093-bbc9-fdbcbbe2f6f5" class="bulleted-list">
                            <li style={{listStyleType: 'disc'}}>
                                æˆ‘
                                <ul id="22ab7849-9a0a-8059-be52-f195000ff9bc" class="bulleted-list">
                                    <li style={{listStyleType: 'circle'}}>è®€+å¯« paper</li>
                                </ul>
                                <ul id="22ab7849-9a0a-80e0-8019-f3e16d46726f" class="bulleted-list">
                                    <li style={{listStyleType: 'circle'}}>implement method</li>
                                </ul>
                            </li>
                        </ul>
                        <ul id="22ab7849-9a0a-808c-8c58-c16a79e8478c" class="bulleted-list">
                            <li style={{listStyleType: 'disc'}}>
                                éšŠå‹
                                <ul id="22ab7849-9a0a-80f2-bca8-c4ac87fd686b" class="bulleted-list">
                                    <li style={{listStyleType: 'circle'}}>è™•ç†å„ç¨® env. çš„æ¯›ç—…</li>
                                </ul>
                                <ul id="22ab7849-9a0a-801d-9c3f-c95f2ddb9430" class="bulleted-list">
                                    <li style={{listStyleType: 'circle'}}>implement method</li>
                                </ul>
                            </li>
                        </ul>
                        <p id="22ab7849-9a0a-80eb-a579-e70a57b319f8" class="">é€™ç®—æ˜¯å¤§æ¦‚çš„åˆ†å·¥ï¼Œå…¶å¯¦å…©å€‹äººå¤§éƒ¨åˆ†éƒ½æœ‰è™•ç†åˆ°æ‰€æœ‰ taskï¼Œå¤§æ¦‚æ˜¯è‡ªå·±çš„ 70% + éšŠå‹çš„ 30% å§ã€‚</p>
                        <h3 id="22ab7849-9a0a-80d1-9165-fa5f5c9836c3" class="">æŠ•ç¨¿</h3>
                        <p id="22ab7849-9a0a-8057-8c70-feb91c22b9db" class="">æˆ‘å€‘å…¶å¯¦æœ€ä¸€é–‹å§‹ç›®æ¨™åªæœ‰å¤§ conference çš„ workshopï¼Œçµæœä¸çŸ¥é“ç‚ºä»€éº¼å¯«å°±è·‘å»æŠ• conference äº†ã€‚æŠ•äº† <a href="https://www.iros25.org/">2025 IEEE IROS</a>ï¼Œç®—æ˜¯ IEEE ä¸‰å¤§ robotics conference ä¹‹ä¸€ï¼ŒA ç´šé ‚ç´š conferenceã€‚</p>
                        <p id="22ab7849-9a0a-80e0-ab9d-e993217d0012" class="">çµæœå°±ä¸æ„å¤–çš„è¢« rejectï¼Œæˆ‘åªèƒ½èªªæˆ‘è¦ºå¾—å¤§éƒ¨åˆ†è¬›çš„é»éƒ½å¾ˆæœ‰é“ç†ï¼Œç•¢ç«Ÿ limited novelty å¥½åƒæ˜¯æŠ•é ‚ç´šæœŸåˆŠæ™‚æˆ‘å€‘é€™ç¨®å°èœé›æœƒé¢å°çš„å•é¡Œï¼Œåªèƒ½å°±ç¹¼çºŒæŠ•èœä¸€é»çš„æœƒå˜ (å¦‚æœæˆ‘æœ‰ç©ºçš„è©±)ã€‚</p>
                        <p id="22ab7849-9a0a-80b8-be6c-ec8fe0e63148" class="">é †ä¾¿é™„ä¸Š reviewer å€‘çš„æŒ‡æ•™</p>
                        <ul id="22ab7849-9a0a-8085-b367-d8d02708cc9d" class="toggle">
                            <li>
                                <details open="">
                                    <summary>Reviewer 1</summary>
                                    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script>
                                    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/>
                                    <pre id="22ab7849-9a0a-80d7-b783-c76a266bce5d" class="code"><code class="language-Plain Text" style={{whiteSpace: 'pre-wrap', wordBreak: 'break-all'}}>Reviewer 1 of IROS 2025 submission 4450<br/>
                <br/>
                Comments to the author<br/>
                <br/>
                This paper focuses on the application of Multimodal Large
                Language Models (MLLMs) in food preparation task planning.
                It addresses challenges like cross-modal distraction and
                geometric feasibility and proposes an effective approach to
                improve MLLM performance.<br/>
                <br/>
                ### Core Methodology<br/>
                The paper&#x27;s core methodology is centered around a
                three-staged planning pipeline for food preparation task
                planning using MLLMs. First, in the MLLM Planning stage,
                zeroshot CoT prompting is employed to generate subgoal
                explanations and descriptions before deciding on the final
                skill sequence, enhancing its reasoning ability. Next,
                Self-Consistency verification resolve conflicts and
                stabilize skill selection. Finally, Skill Affordance and
                Replanning focus on geometric feasibility. If an infeasible
                skill is chosen, replanning is triggered with the help of
                structured feedback from the affordance module, improving
                the MLLM&#x27;s awareness of geometric constraints.<br/>
                <br/>
                ### Core Contributions
                The paper makes several key contributions. It constructs a
                unique dataset for food preparation task planning, which
                includes five categories of tasks. This dataset is used to
                comprehensively evaluate various robotic capabilities such
                as semantic reasoning, quantity estimation, and collision
                avoidance. Secondly, it adapts CoT with Self - Consistency
                to closed - loop task planning. This adaptation mitigates
                the reasoning loss caused by cross - modal distractions,
                making the MLLM more robust. Finally, the paper builds
                action preconditions with predicates. This enables the MLLM
                to consider geometric feasibility during task planning,
                ensuring that the planned actions are physically executable
                in the given environment.<br/>
                <br/>
                <br/>
                The good part of the paper is that, it provides a good
                figure demonstrating the whole pipeline, and it provides
                prompt examples and skill definitions, but there also exist
                major limitations in this paper:<br/>
                <br/>
                1. The paper only work with existing LLMs, without any
                training or even finetuning. This makes the work much less
                useful, as prompt engineering cannot be a novel thing today<br/>
                2. The experiment makes me confused. Naive LLM works better
                than naive MLLM? How can an agent accomplish a task without
                perception of the world? The author should further clarify
                what is the experiment setting for evaluating naive LLM
                (input, prompts, etc).<br/>
                3. Lack of novelty and generalizability: the paper proposed
                nothing new to solve the task planning in food preparation.
                Self-consistency and skill deconstruction are well-explored
                techniques, geometric feasiblity validation is also basic
                in robotics field. The whole pipeline only works for food
                preparation tasks, and it&#x27;s only tested in the simulated
                environment in IsaacGym. The author should conduct more
                experiments under more diverse settings.<br/>
                <br/>
                Overall, the paper is written with good quality, but it
                lacks novel approach designs or training know-hows. It&#x27;s
                also short with only around 5 pages of content. It&#x27;s under
                the borderline of IROS in my opinion.<br/>
                <br/>
                <br/>
                ### Pros<br/>
                1. **Comprehensive Problem Solving**: Addresses two key
                challenges, cross-modal distraction and geometric
                feasibility, which are crucial for practical application of
                MLLMs in robotics. The proposed approach combines multiple
                techniques to effectively improve MLLM performance in food
                preparation tasks.<br/>
                2. **Effective Evaluation**: The construction of a
                dedicated dataset with diverse task categories provides a
                comprehensive way to evaluate MLLMs. The comparison with
                multiple baselines clearly demonstrates the superiority of
                the proposed method, offering valuable insights into
                different approaches.<br/>
                3. **Practical Significance**: The research has practical
                implications for real - world applications, such as service
                robots in food - related industries. It helps robots better
                understand and execute natural language instructions in
                complex environments.<br/>
                <br/>
                ### Cons<br/>
                1. **Simplified Control Policies**: Relies on simplified
                policies, which limits task diversity. For example, the
                scooping policy cannot specify quantities, restricting the
                scope of tasks the robot can handle.<br/>
                2. **Dependence on Object Detection**: Assumes a reliable
                object detector but does not account for detector failures.
                This may lead to mismatches between semantic input and
                visual observations, affecting the model&#x27;s performance in
                real - world scenarios.<br/>
                3. **Limitations in MLLM Understanding**: Although MLLMs
                have general image recognition abilities, they still
                misinterpret quantities and spatial relationships. The
                current method enforces consistency verification instead of
                directly improving the MLLM&#x27;s reasoning ability in these
                aspects.<br/>
                <br/>
                Comments on the Video Attachment<br/>
                ================================<br/>
                <br/>
                The video is more like a animation of the figure 2, it&#x27;s
                better to put some query and real responses from LLMs</code></pre>
                                </details>
                            </li>
                        </ul>
                        <ul id="22ab7849-9a0a-806a-9e99-f187d9937857" class="toggle">
                            <li>
                                <details open="">
                                    <summary>Reviewer 2</summary>
                                    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script>
                                    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/>
                                    <pre id="22ab7849-9a0a-80b0-ac9d-efaa1a2103ff" class="code"><code class="language-Plain Text" style={{whiteSpace: 'pre-wrap', wordBreak: 'break-all'}}>Reviewer 5 of IROS 2025 submission 4450<br/>
                <br/>
                Comments to the author<br/>
                ======================<br/>
                <br/>
                This paper focuses on using LLMs for a subtask of robot
                manipulators, which is food preparation. The paper topic is
                quite interesting, however, the paper significantly lacks
                technical novelty and contributions.<br/>
                <br/>
                Although presented as a three-fold contribution by the
                authors in the first section, the only contribution is the
                planning pipeline illustrated in Fig. 2, which is an
                ensemble of multiple obvious sub-tasks.<br/>
                <br/>
                As for the dataset that the authors developed, limited
                information is shared regarding how readers can access it
                for their research.<br/>
                <br/>
                Additionally, the results shown in TABLE I do not include
                any comparisons with any state-of-the-art approaches, which
                seems to be an ablation study instead.<br/>
                <br/>
                Overall, I do not recommend this paper for acceptance.<br/>
                <br/>
                Comments on the Video Attachment<br/>
                ================================<br/>
                <br/>
                N/A</code></pre>
                                </details>
                            </li>
                        </ul>
                        <ul id="22ab7849-9a0a-80be-a6b9-ed02f32c5ddf" class="toggle">
                            <li>
                                <details open="">
                                    <summary>Reviewer 3</summary>
                                    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js" integrity="sha512-7Z9J3l1+EYfeaPKcGXu3MS/7T+w19WtKQY/n+xzmw4hZhJ9tyYmcUS+4QqAlzhicE5LAfMQSF3iFTK9bQdTxXg==" crossorigin="anonymous" referrerPolicy="no-referrer"></script>
                                    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" integrity="sha512-tN7Ec6zAFaVSG3TpNAKtk4DOHNpSwKHxxrsiw4GHKESGPs5njn/0sMCUMl2svV4wo4BK/rCP7juYz+zx+l6oeQ==" crossorigin="anonymous" referrerPolicy="no-referrer"/>
                                    <pre id="22ab7849-9a0a-8056-b076-c5fc7cdde80e" class="code"><code class="language-Plain Text" style={{whiteSpace: 'pre-wrap', wordBreak: 'break-all'}}>Reviewer 6 of IROS 2025 submission 4450<br/>
                <br/>
                Comments to the author<br/>
                ======================<br/>
                <br/>
                The authors identity two important gaps in the embodied AI
                area which is cross-modal distraction and inability to
                analysis geometric feasibility. However, the method
                proposed is not promising enough to convince me to that can
                solve these gaps. In addition, the description of each
                module in the pipeline is quite vague, which also weakens
                the plausibility of the approach. Following are some detail
                comments:<br/>
                <br/>
                1. Related work section: It would be better if the authors
                write a few more sentences to introduce the methods
                addressing those two gaps. MLLM and CoT are cliche.<br/>
                2. Method section C. Zero-shot CoT: What do the &quot;J.&quot;, &quot;L.&quot;,
                &quot;A.&quot;, etc. in your demo output stand for?<br/>
                3. Method section D. Self-Consistency Verification: You
                write &quot;If the MLLM chooses move_to_white_bowl and it is
                validated by the Skill Affordance module, ...&quot;, what if the
                MLLM choose &quot;scoop&quot;? Will you fine tune or somehow punish
                the MLLM so it will improve next time?<br/>
                4. Method section E. Skill Affordance and Replan: â€œTo
                enable the MLLM to assess the skill affordance, we design
                predicates [8], ...â€ so your predicates are from [8], which
                means it is not your novel idea? So what is your
                contribution?<br/>
                5. Result and Discussion section: Table 1, you need to cite
                the exact method you choose as baselines in the table, so
                that the readers can understand how your work performs
                compared to other state-of-the-art techniques.<br/>
                <br/>
                Comments on the Video Attachment<br/>
                ================================<br/>
                <br/>
                The video clearly demonstrate the performance of their
                proposed method following good explanation about the
                details of proposed method.</code></pre>
                                </details>
                            </li>
                        </ul>
                        <p id="22ab7849-9a0a-800e-9fa6-f3171c741906" class="">Reviewer 1 è¬›çš„å¾ˆä¸­è‚¯ï¼Œåæ­£æˆ‘æ˜¯å¿ƒæœå£æœã€‚Reviewer 2 ä¸€å‰¯æ²’èªçœŸçœ‹ paperã€‚Reviewer 3 çœ‹èµ·ä¾†æœ‰èªçœŸçœ‹ä½†æ²’çœ‹æ‡‚ã€‚å…¶å¯¦æœ‰äº›æ±è¥¿å¦‚æœæœ‰ rebuttal çš„è©±å¯ä»¥è¬›æ¸…æ¥šï¼Œä½† IROS æ²’æœ‰â€¦</p>
                        <p id="22ab7849-9a0a-80f8-963d-ed10dec25fe7" class="">ğŸ¥³ ã„é‚£å€‹ reviewer 1 èªª the paper is written with good quality ã„ ğŸ¥³</p>
                        <h3 id="22ab7849-9a0a-802b-b4bd-de17275fc1ae" class="">å¿ƒå¾—</h3>
                        <ul id="22ab7849-9a0a-8051-8fa3-e3465a96dfd3" class="bulleted-list">
                            <li style={{listStyleType: 'disc'}}>
                                <strong>Research</strong>
                                <p id="22ab7849-9a0a-80cf-8218-f3e41024daa0" class="">é€™æ˜¯æˆ‘ç¬¬ä¸€æ¬¡åšç ”ç©¶ï¼Œéç¨‹ç®—æ˜¯éå¸¸åå·ã€‚åœ¨å°ˆé¡Œæœ€ä¸€é–‹å§‹çš„æ™‚å€™ï¼ŒYT å¾ˆç›´ç™½çš„è·Ÿæˆ‘å€‘èªªä»–å®Œå…¨æ²’åšéé€™å€‹æ–¹å‘ï¼Œæˆ‘å€‘æœƒèŠ±å¾ˆå¤šæ™‚é–“è‡ªå·±æ‘¸ç´¢ã€‚LLM ç›¸é—œç ”ç©¶ç®—æ˜¯é‚£å¹¾å¹´æ‰æœ‰çš„ï¼Œæ‰€ä»¥å¾ˆå¤§ä¸€éƒ¨åˆ†éƒ½æ˜¯æˆ‘å€‘åœ¨çœ‹ paper æ‰¾æ–¹å‘ï¼Œå¶çˆ¾ YT çœ‹åˆ°ä»€éº¼æ–¹å‘æ‰æœƒå‚³çµ¦æˆ‘å€‘è®“æˆ‘å€‘å»è©¦çœ‹çœ‹ (é›–ç„¶æ˜¯æ­»è·¯ * n)ï¼Œå‰ä¸­æœŸæ˜¯çœŸçš„ç¢°å£ç¢°çˆ›ã€‚æˆ‘å€‘ç®—æ˜¯å¾ˆå®Œæ•´çš„ç¶“æ­·äº†ä¸€å€‹å¾å®šæ–¹å‘ã€å®šé¡Œç›®ã€formulate problemã€æ‰¾ solutionã€å¯« paper çš„ç ”ç©¶éç¨‹ã€‚é›–ç„¶æœ€å¾ŒæŠ•ç¨¿æ²’ä¸Šï¼Œä½†æˆ‘è¦ºå¾—é€™å·²ç¶“æ˜¯ä¸€å€‹å°æˆ‘è€Œè¨€å¾ˆå¥½çš„ç¶“é©— + è¨“ç·´äº†ã€‚</p>
                            </li>
                        </ul>
                        <ul id="22ab7849-9a0a-800d-aaf8-e842da030a0d" class="bulleted-list">
                            <li style={{listStyleType: 'disc'}}>
                                <strong>å¯« (è¶•) paper</strong>
                                <p id="22ab7849-9a0a-800f-8226-e5584fa4f8df" class="">é ‚ç´š paper guideline -  <a href="https://perceiving-systems.blog/post/writing-a-good-scientific-paper">Writing a good scientific paper</a></p>
                                <p id="22ab7849-9a0a-80ab-a1c3-ce86d3d8f194" class="">å¼·èª¿ä¸€ä¸‹ï¼Œ<strong>ä¸è¦åœ¨ deadline å‰ç˜‹ç‹‚åšå¯¦é©— + å¯« paper</strong>ï¼Œå¾ˆç´¯å¾ˆæ“ï¼ŒåŒæ™‚è¦æ•´ç†çµæœ + å¯« paper çœŸçš„æ˜¯ä¸€ä»¶éå¸¸æ¶ˆè€—å¿ƒåŠ›çš„äº‹ã€‚</p>
                                <p id="22ab7849-9a0a-80d4-ae89-f87769f243e9" class=""><strong>æ•´ç†å¯¦é©—</strong>çµæœæ˜¯ä¸€å€‹éå¸¸æ¯ç‡¥åˆè€ƒé©—è§€å¯ŸåŠ›çš„ taskï¼Œç‚ºäº†ä¿æŒ metric çš„ä¸€è‡´ï¼Œå…¨éƒ¨å¯¦é©—çµæœéƒ½æ˜¯æˆ‘æ•´ç†çš„ï¼Œåœ¨æ•´ç†çš„æ™‚å€™é‚„è¦çœ‹ä¸€ä¸‹ failure case æ˜¯ä¸æ˜¯å› ç‚ºæœ‰ä¸€äº›æ±è¥¿æ˜¯æˆ‘å€‘æ²’è¨­å¥½è€Œä¸æ˜¯æ–¹æ³•æœ¬èº«çš„å•é¡Œï¼Œé‚„è¦å»åˆ†æç‚ºä»€éº¼æœƒæœ‰é€™äº›å•é¡Œ (æˆ‘ discussion å¯«å¾—å¾ˆè¾›è‹¦ &amp; æˆ‘è¦ºå¾—æˆ‘ evaulation åšå¾—å¾ˆå¥½ï¼Œä½† reviewer ä¼¼ä¹å«Œå°‘â€¦)ã€‚</p>
                                <p id="22ab7849-9a0a-80f2-ac0a-c74619b25335" class=""><strong>å¯« paper</strong> æ›´ç´¯ï¼Œè¦å¯«çš„ç°¡çŸ­åˆæ˜ç¢ºçœŸçš„æ˜¯ä¸€ä»¶éœ€è¦å¤šæ¬¡ä¿®ä¿®æ”¹æ”¹çš„äº‹ã€‚å°æˆ‘ä¾†èªªæœ€é›£çš„æ˜¯ introductionï¼Œæˆ‘ä¸€é–‹å§‹å¯«äº†ä¸€ç‰ˆæˆ‘è¦ºå¾—é‚„ç®—æ»¿æ„çš„æ•…äº‹ï¼Œçµæœ YT åœ¨æœ€å¾Œä¸€å¤©å¹¾ä¹æŠŠä»–æŒªæˆä¸€å€‹æˆ‘ä¸èªè­˜çš„æ¨£å­ï¼Œè€Œä¸”çœ‹èµ·ä¾†æ¯”æˆ‘åŸæœ¬å¯«çš„é‚è¼¯æ›´æ¸…æ™°ä¹Ÿæ›´æœ‰è„ˆçµ¡ï¼Œæˆ‘é‚£æ™‚å€™å¯«äº†å¿«ä¸€é€±çš„ intro çµæœ YT ä¸€æ™šä¸Šå¼„ä¸€å€‹æ¯”æˆ‘åŸæœ¬çš„ç‰›é€¼çš„ï¼Œè¶…æŒ«æŠ˜ç¬‘æ­»ï¼Œä½†æ²’è¾¦æ³•å¯« paper èƒ½åŠ›è¼¸æ•™æˆéå¸¸æ­£å¸¸ (æ„Ÿè¬ YT æä¾›é¢è©¦æŒ«æŠ˜é¡Œç´ æ)ã€‚å¯«å®Œæ•´ç¯‡ paper ä¹‹å¾Œé “æ™‚æœ‰ç¨®é€™ç¯‡ paper æ˜¯æˆ‘å…’å­çš„æ„Ÿè¦ºï¼Œæ˜¯çœŸçš„ç‚ºäº†ä»–çš„èª•ç”Ÿæ²’æ—¥æ²’å¤œï¼Œæ„Ÿè¬ YTã€å¬¿ç²ã€å¥•å„’ã€æ¬Šç¥ for all the support!!!</p>
                            </li>
                        </ul>
                    </div>
                </article>
			</React.Fragment>
		),
	};
}

export default HCISPaper;
